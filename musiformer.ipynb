{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11488362,"sourceType":"datasetVersion","datasetId":7201044},{"sourceId":11488613,"sourceType":"datasetVersion","datasetId":7201221},{"sourceId":11491128,"sourceType":"datasetVersion","datasetId":7203217}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch.optim import AdamW, Adam\nfrom torch.utils.data import DataLoader, Dataset\nimport os\nfrom torch.nn.functional import one_hot\n\nimport torch\nimport torch.nn as nn\n\nimport joblib\nimport numpy as np\nimport math","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T20:24:22.227871Z","iopub.execute_input":"2025-04-20T20:24:22.228122Z","iopub.status.idle":"2025-04-20T20:24:26.228025Z","shell.execute_reply.started":"2025-04-20T20:24:22.228100Z","shell.execute_reply":"2025-04-20T20:24:26.227260Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data preparation","metadata":{}},{"cell_type":"code","source":"entokened = joblib.load(\"/kaggle/input/tokenized-test2/tokenized_test2.pkl\")\nprint(len(entokened))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T20:24:26.229159Z","iopub.execute_input":"2025-04-20T20:24:26.229569Z","iopub.status.idle":"2025-04-20T20:24:26.590888Z","shell.execute_reply.started":"2025-04-20T20:24:26.229548Z","shell.execute_reply":"2025-04-20T20:24:26.590170Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random as rd\ndef load_and_tokenized(file):\n    return file\n\nclass MyDataset(Dataset):\n    def __init__(self):\n        self.files = entokened\n\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, index):\n        return torch.tensor(self.files[index])[:rd.randint(100, 2000)]\n\ndef my_collate_fn(batch):\n    sequences = batch\n\n    max_len = max([seq.size(0) for seq in sequences])\n\n    padded_sequences = []\n    padding_value = 2617\n\n    for seq in sequences:\n        padding_needed = max_len - seq.size(0)\n        padded_seq = torch.nn.functional.pad(seq, (0, padding_needed), value=padding_value)\n        padded_sequences.append(padded_seq)\n\n    batch_tensor = torch.stack(padded_sequences, dim=0)\n    return batch_tensor\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T20:24:28.546136Z","iopub.execute_input":"2025-04-20T20:24:28.546661Z","iopub.status.idle":"2025-04-20T20:24:28.552210Z","shell.execute_reply.started":"2025-04-20T20:24:28.546636Z","shell.execute_reply":"2025-04-20T20:24:28.551547Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = MyDataset()\ndataloader = DataLoader(dataset, batch_size=4, collate_fn=my_collate_fn)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T20:24:31.419777Z","iopub.execute_input":"2025-04-20T20:24:31.420526Z","iopub.status.idle":"2025-04-20T20:24:31.424155Z","shell.execute_reply.started":"2025-04-20T20:24:31.420487Z","shell.execute_reply":"2025-04-20T20:24:31.423431Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"next(iter(dataloader)).shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T20:24:33.051109Z","iopub.execute_input":"2025-04-20T20:24:33.051407Z","iopub.status.idle":"2025-04-20T20:24:33.111580Z","shell.execute_reply.started":"2025-04-20T20:24:33.051356Z","shell.execute_reply":"2025-04-20T20:24:33.110836Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T20:24:34.576887Z","iopub.execute_input":"2025-04-20T20:24:34.577290Z","iopub.status.idle":"2025-04-20T20:24:34.656537Z","shell.execute_reply.started":"2025-04-20T20:24:34.577255Z","shell.execute_reply":"2025-04-20T20:24:34.655867Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TransformerBlock(nn.Module):\n    \"\"\"\n    A single Transformer block using Pre-LN (Layer Normalization before attention/MLP)\n    and accepting boolean padding mask.\n    \"\"\"\n    def __init__(self, embedding_dimension=128, heads=1, dropout_rate=0.1):\n        super(TransformerBlock, self).__init__()\n        self.att = nn.MultiheadAttention(embed_dim=embedding_dimension, num_heads=heads, dropout=dropout_rate, batch_first=True)\n        self.norm1 = nn.LayerNorm(embedding_dimension)\n        self.norm2 = nn.LayerNorm(embedding_dimension)\n        self.MLP = nn.Sequential(\n            nn.Linear(embedding_dimension, embedding_dimension * 4), \n            nn.GELU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(embedding_dimension * 4, embedding_dimension),\n            nn.Dropout(dropout_rate)\n        )\n        self.dropout = nn.Dropout(dropout_rate)\n\n    def forward(self, x, key_padding_mask=None):\n        seq_len = x.size(1)\n\n        norm_x = self.norm1(x)\n\n        causal_mask = nn.Transformer.generate_square_subsequent_mask(seq_len, device=device)\n        \n\n        attn_output, _ = self.att(norm_x, norm_x, norm_x,\n                                  key_padding_mask=key_padding_mask.to(causal_mask.device),\n                                  is_causal=True,\n                                  need_weights=False,\n                                  attn_mask=causal_mask)\n        \n        x = x + self.dropout(attn_output) \n\n        norm_x = self.norm2(x)\n        mlp_output = self.MLP(norm_x)\n        out = x + mlp_output \n        return out\n\nclass T1(nn.Module):\n    def __init__(self, vocab_size, embedding_dimension=128, heads=1, num_transformer_blocks=1, padding_token=0, dropout_rate=0.1):\n        super(T1, self).__init__()\n        self.padding_token = padding_token\n        self.embedding = nn.Embedding(vocab_size, embedding_dimension, padding_idx=padding_token) \n        self.pos_embedder = nn.Embedding(3000, embedding_dimension)\n\n        self.transformer_blocks = nn.ModuleList([\n            TransformerBlock(embedding_dimension=embedding_dimension, heads=heads, dropout_rate=dropout_rate)\n            for _ in range(num_transformer_blocks)\n        ])\n\n        self.final_norm = nn.LayerNorm(embedding_dimension) \n        self.unembedder = nn.Linear(embedding_dimension, vocab_size)\n        # self.last_act = nn.Sigmoid()\n\n\n    def forward(self, input_seq):\n        if input_seq.device != self.embedding.weight.device:\n             input_seq = input_seq.to(self.embedding.weight.device)\n             \n        padding_mask = (input_seq == self.padding_token) \n        x = self.embedding(input_seq) * math.sqrt(self.embedding.embedding_dim) \n        x += self.pos_embedder(torch.arange(input_seq.shape[1]).to(x.device).unsqueeze(0))\n\n        for block in self.transformer_blocks:\n            x = block(x, key_padding_mask=padding_mask) \n\n        x = self.final_norm(x)\n\n        logits = self.unembedder(x)\n        #logits = self.last_act(logits)\n\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T20:24:35.947005Z","iopub.execute_input":"2025-04-20T20:24:35.947312Z","iopub.status.idle":"2025-04-20T20:24:35.977114Z","shell.execute_reply.started":"2025-04-20T20:24:35.947290Z","shell.execute_reply":"2025-04-20T20:24:35.976239Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vocab_size = 3151\n\nmodel = T1(vocab_size=vocab_size, embedding_dimension=250, heads=5, num_transformer_blocks=16, padding_token=3150)\nmodel = model.to(device)\nif torch.cuda.device_count() > 1:\n    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n    model = nn.DataParallel(model).cuda()\n\noptimizer = Adam(model.parameters(), lr=0.001)\nloss_fn = torch.nn.CrossEntropyLoss()\nepochs = 100\ntotal_params = sum(p.numel() for p in model.parameters())\ntotal_params\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T20:25:10.747913Z","iopub.execute_input":"2025-04-20T20:25:10.748170Z","iopub.status.idle":"2025-04-20T20:25:10.877490Z","shell.execute_reply.started":"2025-04-20T20:25:10.748150Z","shell.execute_reply":"2025-04-20T20:25:10.876934Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc\ngc.collect()\ntorch.cuda.empty_cache()\n\n#del pred\ndel losses\ndel model\ndel optimizer\ndel batch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T20:25:09.276662Z","iopub.execute_input":"2025-04-20T20:25:09.277240Z","iopub.status.idle":"2025-04-20T20:25:09.398292Z","shell.execute_reply.started":"2025-04-20T20:25:09.277218Z","shell.execute_reply":"2025-04-20T20:25:09.397554Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for epoch in range(epochs):\n    losses = []\n    for i, batch in enumerate(dataloader):\n        optimizer.zero_grad()\n        batch = batch.to(device)\n        preds = model(batch[:, :-1])\n\n        batch_size, sequence_length, vocab_size = preds.shape\n\n        #print(preds.shape)\n        loss = loss_fn(preds.view(-1, vocab_size), batch[:, 1:].contiguous().view(-1))\n\n        loss.backward()\n        optimizer.step()\n\n        losses.append(loss.item())\n        if i % 5 == 0:\n             print(f\"epoch: {epoch}, loss: {loss.item():.4f}\")\n    print(f\"Epoch {epoch+1} Mean Loss: {np.mean(losses):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T20:33:11.801068Z","iopub.execute_input":"2025-04-20T20:33:11.801594Z","iopub.status.idle":"2025-04-20T20:46:02.606989Z","shell.execute_reply.started":"2025-04-20T20:33:11.801568Z","shell.execute_reply":"2025-04-20T20:46:02.606103Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"example = dataset[0]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T20:46:45.210333Z","iopub.execute_input":"2025-04-20T20:46:45.210649Z","iopub.status.idle":"2025-04-20T20:46:45.214498Z","shell.execute_reply.started":"2025-04-20T20:46:45.210627Z","shell.execute_reply":"2025-04-20T20:46:45.213713Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for _ in range(60):\n    with torch.no_grad():\n        pred = model(example.unsqueeze(0).to(device))[0, -1, :]\n        pred = torch.argmax(pred, dim=-1).cpu()\n    \n    example = torch.cat((example, pred.unsqueeze(0)))\nexample","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T17:47:24.156722Z","iopub.execute_input":"2025-04-20T17:47:24.157389Z","iopub.status.idle":"2025-04-20T17:47:25.335000Z","shell.execute_reply.started":"2025-04-20T17:47:24.157348Z","shell.execute_reply":"2025-04-20T17:47:25.334168Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.distributions import Categorical\n\nexample = example.unsqueeze(0)\nfor _ in range(40):\n    with torch.no_grad():\n        logits = model(example.to(device))\n        probabilities = torch.nn.functional.softmax(logits, dim=-1)\n        batch_size, sequence_length, vocab_size = logits.shape\n        probabilities_flat = probabilities.view(-1, vocab_size)\n        dist = Categorical(probabilities_flat)\n        sampled_indices_flat = dist.sample()\n        pred = sampled_indices_flat.view(batch_size, sequence_length)[:, -1:]\n        pred = pred.cpu()\n        \n    #print(example.shape, pred.shape)\n    example = torch.cat((example, pred), dim=1)\nexample","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T20:46:47.034205Z","iopub.execute_input":"2025-04-20T20:46:47.034498Z","iopub.status.idle":"2025-04-20T20:46:47.563327Z","shell.execute_reply.started":"2025-04-20T20:46:47.034475Z","shell.execute_reply":"2025-04-20T20:46:47.562764Z"}},"outputs":[],"execution_count":null}]}