{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11488362,"sourceType":"datasetVersion","datasetId":7201044},{"sourceId":11488613,"sourceType":"datasetVersion","datasetId":7201221},{"sourceId":11491128,"sourceType":"datasetVersion","datasetId":7203217}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch.optim import AdamW, Adam\nfrom torch.utils.data import DataLoader, Dataset\nimport os\nfrom torch.nn.functional import one_hot\n\nimport torch\nimport torch.nn as nn\n\nimport joblib\nimport numpy as np\nimport math","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-22T08:07:44.567344Z","iopub.execute_input":"2025-04-22T08:07:44.567597Z","iopub.status.idle":"2025-04-22T08:07:44.589246Z","shell.execute_reply.started":"2025-04-22T08:07:44.567578Z","shell.execute_reply":"2025-04-22T08:07:44.588792Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Data preparation","metadata":{}},{"cell_type":"code","source":"entokened = joblib.load(\"/kaggle/input/tokenized-test3/tokenized_test3.pkl\")\nprint(len(entokened))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T08:07:48.145319Z","iopub.execute_input":"2025-04-22T08:07:48.145955Z","iopub.status.idle":"2025-04-22T08:07:48.519935Z","shell.execute_reply.started":"2025-04-22T08:07:48.145934Z","shell.execute_reply":"2025-04-22T08:07:48.519198Z"}},"outputs":[{"name":"stdout","text":"512\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import random as rd\ndef load_and_tokenized(file):\n    return file\n\nclass MyDataset(Dataset):\n    def __init__(self):\n        self.files = entokened\n\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, index):\n        return torch.tensor(self.files[index])[:2000]\n\ndef my_collate_fn(batch):\n    sequences = batch\n\n    max_len = max([seq.size(0) for seq in sequences])\n\n    padded_sequences = []\n    padding_value = 2699\n\n    for seq in sequences:\n        padding_needed = max_len - seq.size(0)\n        padded_seq = torch.nn.functional.pad(seq, (0, padding_needed), value=padding_value)\n        padded_sequences.append(padded_seq)\n\n    batch_tensor = torch.stack(padded_sequences, dim=0)\n    return batch_tensor\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T08:07:50.248616Z","iopub.execute_input":"2025-04-22T08:07:50.248872Z","iopub.status.idle":"2025-04-22T08:07:50.259505Z","shell.execute_reply.started":"2025-04-22T08:07:50.248853Z","shell.execute_reply":"2025-04-22T08:07:50.258663Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"dataset = MyDataset()\ndataloader = DataLoader(dataset, batch_size=6, collate_fn=my_collate_fn)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T08:07:55.380752Z","iopub.execute_input":"2025-04-22T08:07:55.381015Z","iopub.status.idle":"2025-04-22T08:07:55.384781Z","shell.execute_reply.started":"2025-04-22T08:07:55.380996Z","shell.execute_reply":"2025-04-22T08:07:55.384190Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"print(dataset[10].shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T08:07:57.334648Z","iopub.execute_input":"2025-04-22T08:07:57.334921Z","iopub.status.idle":"2025-04-22T08:07:57.356120Z","shell.execute_reply.started":"2025-04-22T08:07:57.334900Z","shell.execute_reply":"2025-04-22T08:07:57.355506Z"}},"outputs":[{"name":"stdout","text":"torch.Size([1322])\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T08:07:58.816623Z","iopub.execute_input":"2025-04-22T08:07:58.817317Z","iopub.status.idle":"2025-04-22T08:07:58.903255Z","shell.execute_reply.started":"2025-04-22T08:07:58.817255Z","shell.execute_reply":"2025-04-22T08:07:58.902546Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"class TransformerBlock(nn.Module):\n    \"\"\"\n    A single Transformer block using Pre-LN (Layer Normalization before attention/MLP)\n    and accepting boolean padding mask.\n    \"\"\"\n    def __init__(self, embedding_dimension=128, heads=1, dropout_rate=0.1):\n        super(TransformerBlock, self).__init__()\n        self.att = nn.MultiheadAttention(embed_dim=embedding_dimension, num_heads=heads, dropout=dropout_rate, batch_first=True)\n        self.norm1 = nn.LayerNorm(embedding_dimension)\n        self.norm2 = nn.LayerNorm(embedding_dimension)\n        self.MLP = nn.Sequential(\n            nn.Linear(embedding_dimension, embedding_dimension * 4), \n            nn.GELU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(embedding_dimension * 4, embedding_dimension),\n            nn.Dropout(dropout_rate)\n        )\n        self.dropout = nn.Dropout(dropout_rate)\n\n    def forward(self, x, key_padding_mask=None):\n        seq_len = x.size(1)\n\n        norm_x = self.norm1(x)\n\n        causal_mask = nn.Transformer.generate_square_subsequent_mask(seq_len, device=device)\n        \n\n        attn_output, _ = self.att(norm_x, norm_x, norm_x,\n                                  key_padding_mask=key_padding_mask.to(causal_mask.device),\n                                  is_causal=True,\n                                  need_weights=False,\n                                  attn_mask=causal_mask)\n        \n        x = x + self.dropout(attn_output) \n\n        norm_x = self.norm2(x)\n        mlp_output = self.MLP(norm_x)\n        out = x + mlp_output \n        return out\n\nclass T1(nn.Module):\n    def __init__(self, vocab_size, embedding_dimension=128, heads=1, num_transformer_blocks=1, padding_token=0, dropout_rate=0.1):\n        super(T1, self).__init__()\n        self.padding_token = padding_token\n        self.embedding = nn.Embedding(vocab_size, embedding_dimension, padding_idx=padding_token) \n        self.pos_embedder = nn.Embedding(6000, embedding_dimension)\n\n        self.transformer_blocks = nn.ModuleList([\n            TransformerBlock(embedding_dimension=embedding_dimension, heads=heads, dropout_rate=dropout_rate)\n            for _ in range(num_transformer_blocks)\n        ])\n\n        self.final_norm = nn.LayerNorm(embedding_dimension) \n        self.unembedder = nn.Linear(embedding_dimension, vocab_size)\n        # self.last_act = nn.Sigmoid()\n\n\n    def forward(self, input_seq):\n        if input_seq.device != self.embedding.weight.device:\n             input_seq = input_seq.to(self.embedding.weight.device)\n             \n        padding_mask = (input_seq == self.padding_token) \n        x = self.embedding(input_seq) * math.sqrt(self.embedding.embedding_dim) \n        x += self.pos_embedder(torch.arange(input_seq.shape[1]).to(x.device).unsqueeze(0))\n\n        for block in self.transformer_blocks:\n            x = block(x, key_padding_mask=padding_mask) \n\n        x = self.final_norm(x)\n\n        logits = self.unembedder(x)\n        #logits = self.last_act(logits)\n\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T08:08:03.479025Z","iopub.execute_input":"2025-04-22T08:08:03.479691Z","iopub.status.idle":"2025-04-22T08:08:03.502996Z","shell.execute_reply.started":"2025-04-22T08:08:03.479655Z","shell.execute_reply":"2025-04-22T08:08:03.502256Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"vocab_size = 2700\n\nmodel = T1(vocab_size=vocab_size, embedding_dimension=250, heads=10, num_transformer_blocks=24, padding_token=2699)\nmodel = model.to(device)\nif torch.cuda.device_count() > 1:\n    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n    model = nn.DataParallel(model).cuda()\n\noptimizer = Adam(model.parameters(), lr=0.0001)\nloss_fn = torch.nn.CrossEntropyLoss()\nepochs = 100\ntotal_params = sum(p.numel() for p in model.parameters())\ntotal_params\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T08:08:59.990810Z","iopub.execute_input":"2025-04-22T08:08:59.991434Z","iopub.status.idle":"2025-04-22T08:09:00.182226Z","shell.execute_reply.started":"2025-04-22T08:08:59.991409Z","shell.execute_reply":"2025-04-22T08:09:00.181639Z"}},"outputs":[{"name":"stdout","text":"Let's use 2 GPUs!\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"20931200"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"import gc\ngc.collect()\ntorch.cuda.empty_cache()\n\ndel pred\ndel losses\n#del model\n#del optimizer\ndel batch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T08:48:40.233325Z","iopub.execute_input":"2025-04-22T08:48:40.233813Z","iopub.status.idle":"2025-04-22T08:48:40.629716Z","shell.execute_reply.started":"2025-04-22T08:48:40.233790Z","shell.execute_reply":"2025-04-22T08:48:40.628851Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/745127053.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#del model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'pred' is not defined"],"ename":"NameError","evalue":"name 'pred' is not defined","output_type":"error"}],"execution_count":15},{"cell_type":"code","source":"for epoch in range(epochs):\n    losses = []\n    for i, batch in enumerate(dataloader):\n        optimizer.zero_grad()\n        batch = batch.to(device)\n        preds = model(batch[:, :-1])\n\n        batch_size, sequence_length, vocab_size = preds.shape\n\n        loss = loss_fn(preds.view(-1, vocab_size), batch[:, 1:].contiguous().view(-1))\n\n        loss.backward()\n        optimizer.step()\n\n        losses.append(loss.item())\n        if i % 100 == 0:\n             print(f\"epoch: {epoch}, loss: {loss.item():.4f}\")\n    print(f\"Epoch {epoch+1} Mean Loss: {np.mean(losses):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T08:09:12.878725Z","iopub.execute_input":"2025-04-22T08:09:12.878981Z","iopub.status.idle":"2025-04-22T08:47:57.624301Z","shell.execute_reply.started":"2025-04-22T08:09:12.878964Z","shell.execute_reply":"2025-04-22T08:47:57.623295Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"epoch: 0, loss: 8.1663\nEpoch 1 Mean Loss: 5.0960\nepoch: 1, loss: 3.7197\nEpoch 2 Mean Loss: 4.4531\nepoch: 2, loss: 3.2841\nEpoch 3 Mean Loss: 4.1813\nepoch: 3, loss: 3.1527\nEpoch 4 Mean Loss: 4.1169\nepoch: 4, loss: 3.1217\nEpoch 5 Mean Loss: 4.0873\nepoch: 5, loss: 3.0956\nEpoch 6 Mean Loss: 4.0531\nepoch: 6, loss: 3.0606\nEpoch 7 Mean Loss: 4.0026\nepoch: 7, loss: 3.0018\nEpoch 8 Mean Loss: 3.9061\nepoch: 8, loss: 2.8833\nEpoch 9 Mean Loss: 3.7433\nepoch: 9, loss: 2.7470\nEpoch 10 Mean Loss: 3.5887\nepoch: 10, loss: 2.6529\nEpoch 11 Mean Loss: 3.4665\nepoch: 11, loss: 2.5805\nEpoch 12 Mean Loss: 3.3708\nepoch: 12, loss: 2.5210\nEpoch 13 Mean Loss: 3.2924\nepoch: 13, loss: 2.4698\nEpoch 14 Mean Loss: 3.2288\nepoch: 14, loss: 2.4218\nEpoch 15 Mean Loss: 3.1752\nepoch: 15, loss: 2.3794\nEpoch 16 Mean Loss: 3.1282\nepoch: 16, loss: 2.3410\nEpoch 17 Mean Loss: 3.0869\nepoch: 17, loss: 2.3058\nEpoch 18 Mean Loss: 3.0520\nepoch: 18, loss: 2.2763\nEpoch 19 Mean Loss: 3.0207\nepoch: 19, loss: 2.2466\nEpoch 20 Mean Loss: 2.9922\nepoch: 20, loss: 2.2215\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/1260405197.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m              \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"epoch: {epoch}, loss: {loss.item():.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":12},{"cell_type":"code","source":"example = dataset[13]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T08:49:05.969980Z","iopub.execute_input":"2025-04-22T08:49:05.970249Z","iopub.status.idle":"2025-04-22T08:49:05.974372Z","shell.execute_reply.started":"2025-04-22T08:49:05.970229Z","shell.execute_reply":"2025-04-22T08:49:05.973583Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"example = torch.tensor([2691])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T08:49:57.178623Z","iopub.execute_input":"2025-04-22T08:49:57.178878Z","iopub.status.idle":"2025-04-22T08:49:57.182644Z","shell.execute_reply.started":"2025-04-22T08:49:57.178859Z","shell.execute_reply":"2025-04-22T08:49:57.181885Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"for _ in range(60):\n    with torch.no_grad():\n        pred = model(example.unsqueeze(0).to(device))[0, -1, :]\n        pred = torch.argmax(pred, dim=-1).cpu()\n    \n    example = torch.cat((example, pred.unsqueeze(0)))\nexample","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T17:47:24.156722Z","iopub.execute_input":"2025-04-20T17:47:24.157389Z","iopub.status.idle":"2025-04-20T17:47:25.335000Z","shell.execute_reply.started":"2025-04-20T17:47:24.157348Z","shell.execute_reply":"2025-04-20T17:47:25.334168Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.distributions import Categorical\n\nexample = example.unsqueeze(0)\nfor _ in range(40):\n    with torch.no_grad():\n        logits = model(example.to(device))\n        probabilities = torch.nn.functional.softmax(logits, dim=-1)\n        batch_size, sequence_length, vocab_size = logits.shape\n        probabilities_flat = probabilities.view(-1, vocab_size)\n        dist = Categorical(probabilities_flat)\n        sampled_indices_flat = dist.sample()\n        pred = sampled_indices_flat.view(batch_size, sequence_length)[:, -1:]\n        pred = pred.cpu()\n        \n    #print(example.shape, pred.shape)\n    example = torch.cat((example, pred), dim=1)\nexample.cpu().numpy().tolist()[-100:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T08:49:59.148764Z","iopub.execute_input":"2025-04-22T08:49:59.149289Z","iopub.status.idle":"2025-04-22T08:50:00.378376Z","shell.execute_reply.started":"2025-04-22T08:49:59.149245Z","shell.execute_reply":"2025-04-22T08:50:00.377749Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"[[2691,\n  1549,\n  1324,\n  1415,\n  1095,\n  871,\n  928,\n  612,\n  1640,\n  1577,\n  1641,\n  1674,\n  1546,\n  1484,\n  1388,\n  908,\n  2645,\n  1747,\n  1682,\n  1469,\n  1953,\n  1730,\n  2019,\n  1445,\n  840,\n  1475,\n  1731,\n  1957,\n  1867,\n  1836,\n  2632,\n  1575,\n  1921,\n  1858,\n  1730,\n  1729,\n  1505,\n  1634,\n  1635,\n  1925,\n  1766]]"},"metadata":{}}],"execution_count":21}]}